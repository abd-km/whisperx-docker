# Production values for whisperx-api on H200 GPUs
# Works with both nginx and traefik ingress controllers
# Usage: helm install whisperx ./whisperx-api -f values-production.yaml

## Replicas for high availability
replicaCount: 2

## WhisperX H200 optimization
whisperx:
  model:
    name: "large-v3"
    computeType: "float16"  # Optimal for H200
    batchSize: 64          # Leverage 141GB HBM3 memory

## GPU configuration for H200
gpu:
  enabled: true
  vendor: nvidia
  count: 1
  memory: "141Gi"
  nodeSelector:
    enabled: true
    # Target H200 nodes specifically
    nvidia.com/gpu.product: "NVIDIA-H200-Tensor-Core-GPU"

## H200-optimized resources
resources:
  requests:
    cpu: "16"
    memory: "64Gi"
    nvidia.com/gpu: 1
  limits:
    cpu: "32"
    memory: "128Gi"
    nvidia.com/gpu: 1

## Persistent storage - use fast NVMe for H200
persistence:
  enabled: true
  storageClass: "fast-ssd"  # Replace with your fast storage class
  accessMode: ReadWriteMany  # Share cache across pods
  size: 100Gi               # Larger cache for production

## Service configuration
service:
  type: ClusterIP
  port: 8000

## Ingress (works with nginx or traefik)
ingress:
  enabled: true
  className: "nginx"  # Change to "traefik" if using Traefik
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    # For nginx:
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    # For traefik (uncomment if using):
    # traefik.ingress.kubernetes.io/router.tls: "true"
    # traefik.ingress.kubernetes.io/router.middlewares: "ai-services-whisperx-api-timeout@kubernetescrd"
  hosts:
    - host: whisperx.yourdomain.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: whisperx-tls
      hosts:
        - whisperx.yourdomain.com

## Node affinity for H200 nodes
nodeSelector:
  nvidia.com/gpu.product: "NVIDIA-H200-Tensor-Core-GPU"
  # Optional: thermal zone for proper cooling
  # thermal-zone: "optimized"

## Tolerations for GPU nodes
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

## Anti-affinity to spread pods across nodes
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - whisperx-api
          topologyKey: kubernetes.io/hostname

## High priority for production workloads
priorityClassName: "high-priority"

## Pod disruption budget for high availability
podDisruptionBudget:
  enabled: true
  minAvailable: 1

